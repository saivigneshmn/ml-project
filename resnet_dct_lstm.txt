import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
from glob import glob
from tqdm.notebook import tqdm
tqdm.pandas()
import cv2, warnings
warnings.filterwarnings('ignore')
import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten, Input, Add, Dropout, LSTM, TimeDistributed, Embedding, RepeatVector, Concatenate, Bidirectional, Convolution2D
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


# Load the images
img_path = '/Users/srisaijishnuedara/Documents/SEMESTER 6/ML PROJECT/image_captioning/data/Images/'
images = glob(img_path+'*.jpg')
print(len(images))
print(images[:5])

# Load the captions
captions = open('/Users/srisaijishnuedara/Documents/SEMESTER 6/ML PROJECT/image_captioning/data/captions.txt','rb').read().decode('utf-8').split('\n')
captions[:5]

# Load ResNet50 model
resnet_model = ResNet50(include_top=True)
resnet_model.summary()


# Extracting features from images using ResNet50
resnet_features = {}
count = 0

for img_path in tqdm(images):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, (224, 224)) # ResNet model requires images of dimensions (224,224,3)
    img = img.reshape(1, 224, 224, 3) # Reshaping image to the dimensions of a single image
    features = resnet_model.predict(img) # Feature extraction from images
    img_name = img_path.split('/')[-1] # Extracting image name
    resnet_features[img_name] = features.flatten() # Flatten the features to make it compatible with concatenation
    count += 1
    # Fetching the features of only 1500 images as using more than 1500 images leads to overloading memory issues
    if count == 1500:
        break
    if count % 50 == 0:
        print(count)
np.save(resnet_features_file, resnet_features)

len(resnet_features)


# Extracting features from images using DCT
dct_features = {}
count = 0

for img_path in tqdm(images):
    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) # Read image in grayscale
    img = cv2.resize(img, (100, 100)) # Resize image to (100, 100)
    img_dct = cv2.dct(np.float32(img)) # Apply DCT
    img_dct_flat = img_dct.flatten() # Flatten DCT coefficients
    img_name = img_path.split('/')[-1] # Extracting image name
    dct_features[img_name] = img_dct_flat
    count += 1
    # Fetching the features of only 1500 images as using more than 1500 images leads to overloading memory issues
    if count == 1500:
        break
    if count % 50 == 0:
        print(count)
np.save(dct_features_file, dct_features)

len(dct_features)

# Preprocessing the captions text
captions = captions[1:]

captions[:5]

captions[8].split(',')[1]

captions_dict = {}

for cap in captions:
    try:
        img_name = cap.split(',')[0]
        caption = cap.split(',')[1]
        # Each image has 5 captions
        if img_name in resnet_features and img_name in dct_features:
            if img_name not in captions_dict:
                captions_dict[img_name] = [caption] # Storing the first caption
            else:
                captions_dict[img_name].append(caption) # Adding the remaining captions
    except:
        break

len(captions_dict)

# Function to preprocess text
def text_preprocess(text):
    modified_text = text.lower() # Converting text to lowercase
    modified_text = 'startofseq ' + modified_text + ' endofseq' # Appending the special tokens at the beginning and ending of text
    return modified_text

# Storing the preprocessed text within the captions dictionary
for key, val in captions_dict.items():
    for item in val:
        captions_dict[key][val.index(item)] = text_preprocess(item)

# Creating vocabulary of the entire text corpus
count_words = dict()
cnt = 1

for key, val in captions_dict.items(): # Iterating through all images with keys as images and their values as 5 captions
    for item in val: # Iterating through all captions for each image
        for word in item.split(): # Iterating through all words in each caption
            if word not in count_words:
                count_words[word] = cnt
                cnt += 1

len(count_words) # Vocab size

# Encoding the text by assigning each word to its corresponding index in the vocabulary i.e. count_words dictionary
for key, val in captions_dict.items():
    for caption in val:
        encoded = []
        for word in caption.split():
            encoded.append(count_words[word])
        captions_dict[key][val.index(caption)] = encoded

# Determining the maximum possible length of text within the entire captions text corpus
max_len = -1

for key, value in captions_dict.items():
    for caption in value:
        if max_len < len(caption):
            max_len = len(caption)

max_len

vocab_size = len(count_words) # Vocab size is the total number of words present in count_words dictionary
vocab_size


# Building a custom generator function to generate input image features, previously generated text and the text to be generated as output
def generator(img, caption):
    n_samples = 0
    X = []
    y_input = []
    y_output = []

    for key, val in caption.items():
        for item in val:
            for i in range(1,len(item)):
                X.append(np.concatenate((img[key], dct_features[key]))) # Appending the input image features from both ResNet50 and DCT
                input_seq = [item[:i]] # Previously generated text to be used as input to predict the next word
                output_seq = item[i] # The next word to be predicted as output
                # Padding encoded text sequences to the maximum length
                input_seq = pad_sequences(input_seq, maxlen=max_len, padding='post', truncating='post')[0]
                # One Hot encoding the output sequence with vocabulary size as the total no. of classes
                output_seq = to_categorical([output_seq], num_classes=vocab_size+1)[0]
                y_input.append(input_seq)
                y_output.append(output_seq)

    return X, y_input, y_output

X, y_in, y_out = generator(resnet_features, captions_dict)


# Converting input and output into Numpy arrays for faster processing
X = np.array(X)
y_in = np.array(y_in, dtype='float64')
y_out = np.array(y_out, dtype='float64')

X.shape, y_in.shape, y_out.shape

# Establishing the model architecture
embedding_len = 128
MAX_LEN = max_len
vocab_size = len(count_words)

# Model for image feature extraction
img_model = Sequential()
img_model.add(Dense(embedding_len, input_shape=(4096,), activation='relu')) # Change input shape to 4096 due to concatenated features
img_model.add(RepeatVector(MAX_LEN))

img_model.summary()

# Model for generating captions from image features
captions_model = Sequential()
captions_model.add(Embedding(input_dim=vocab_size+1, output_dim=embedding_len, input_length=MAX_LEN))
captions_model.add(LSTM(256, return_sequences=True))
captions_model.add(TimeDistributed(Dense(embedding_len)))

captions_model.summary()

# Concatenating the outputs of image and caption models
concat_output = Concatenate()([img_model.output, captions_model.output])
# First LSTM Layer
output = LSTM(units=128, return_sequences=True)(concat_output)
# Second LSTM Layer
output = LSTM(units=512, return_sequences=False)(output)
# Output Layer
output = Dense(units=vocab_size+1, activation='softmax')(output)
# Creating the final model
final_model = Model(inputs=[img_model.input, captions_model.input], outputs=output)
final_model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics='accuracy')
final_model.summary()

# Model training
mc = ModelCheckpoint('image_caption_generator.h5', monitor='accuracy', verbose=1, mode='max', save_best_only=True)

final_model.fit([X, y_in],
                y_out,
                batch_size=512,
                callbacks=mc,
                epochs=100)