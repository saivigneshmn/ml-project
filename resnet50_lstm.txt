import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
from glob import glob
from tqdm.notebook import tqdm
tqdm.pandas()
import cv2, warnings
warnings.filterwarnings('ignore')
import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten, Input, Add, Dropout, LSTM, TimeDistributed, Embedding, RepeatVector, Concatenate, Bidirectional, Convolution2D
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import ModelCheckpoint

# Load the images
img_path = '/Users/srisaijishnuedara/Documents/SEMESTER_6/ML_PROJECT/image_captioning/data/Images/'
images = glob(img_path+'*.jpg')
print(len(images))
print(images[:5])

# Load the captions
captions = open('/Users/srisaijishnuedara/Documents/SEMESTER_6/ML_PROJECT/image_captioning/data/captions.txt','rb').read().decode('utf-8').split('\n')
captions[:5]

inception_model = ResNet50(include_top=True)
inception_model.summary()

last = inception_model.layers[-2].output # Output of the penultimate layer of ResNet model 
model = Model(inputs=inception_model.input,outputs=last)
model.summary()

resnet_features = {}
count = 0

for img_path in tqdm(images):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    img = cv2.resize(img,(224,224)) # ResNet model requires images of dimensions (224,224,3)
    img = img.reshape(1,224,224,3) # Reshaping image to the dimensions of a single image
    features = model.predict(img).reshape(2048,) # Feature extraction from images
    img_name = img_path.split('/')[-1] # Extracting image name
    resnet_features[img_name] = features
    count += 1
    # Fetching the features of only 1500 images as using more than 1500 images leads to overloading memory issues
    if count == 1500:
        break
    if count % 50 == 0:
        print(count)

# Preprocessing the captions text
captions = captions[1:]

captions[:5]

captions[8].split(',')[1]

captions_dict = {}

for cap in captions:
    try:
        img_name = cap.split(',')[0]
        caption = cap.split(',')[1]
        # Each image has 5 captions
        if img_name in resnet_features:
            if img_name not in captions_dict:
                captions_dict[img_name] = [caption] # Storing the first caption
            else:
                captions_dict[img_name].append(caption) # Adding the remaining captions
    except:
        break

len(captions_dict)

# Function to preprocess text
def text_preprocess(text):
    modified_text = text.lower() # Converting text to lowercase
    modified_text = 'startofseq ' + modified_text + ' endofseq' # Appending the special tokens at the beginning and ending of text
    return modified_text

# Storing the preprocessed text within the captions dictionary
for key, val in captions_dict.items():
    for item in val:
        captions_dict[key][val.index(item)] = text_preprocess(item)

count_words = dict()
cnt = 1

for key, val in captions_dict.items(): # Iterating through all images with keys as images and their values as 5 captions
    for item in val: # Iterating through all captions for each image
        for word in item.split(): # Iterating through all words in each caption
            if word not in count_words:
                count_words[word] = cnt
                cnt += 1

len(count_words) # Vocab size

# Encoding the text by assigning each word to its corresponding index in the vocabulary i.e. count_words dictionary
for key, val in captions_dict.items():
    for caption in val:
        encoded = []
        for word in caption.split():
            encoded.append(count_words[word])
        captions_dict[key][val.index(caption)] = encoded
        
# Determining the maximum possible length of text within the entire captions text corpus
max_len = -1

for key, value in captions_dict.items():
    for caption in value:
        if max_len < len(caption):
            max_len = len(caption)

max_len

vocab_size = len(count_words) # Vocab size is the total number of words present in count_words dictionary
vocab_size


def generator(img,caption):
    n_samples = 0
    X = []
    y_input = []
    y_output = []
    
    for key, val in caption.items(): 
        for item in val: 
            for i in range(1,len(item)):
                X.append(img[key]) # Appending the input image features
                input_seq = [item[:i]] # Previously generated text to be used as input to predict the next word 
                output_seq = item[i] # The next word to be predicted as output
                # Padding encoded text sequences to the maximum length
                input_seq = pad_sequences(input_seq,maxlen=max_len,padding='post',truncating='post')[0] 
                # One Hot encoding the output sequence with vocabulary size as the total no. of classes
                output_seq = to_categorical([output_seq],num_classes=vocab_size+1)[0]
                y_input.append(input_seq)
                y_output.append(output_seq)
    
    return X, y_input, y_output

X, y_in, y_out = generator(resnet_features,captions_dict)


len(X), len(y_in), len(y_out)


# Converting input and output into Numpy arrays for faster processing
X = np.array(X)
y_in = np.array(y_in,dtype='float64')
y_out = np.array(y_out,dtype='float64')

X.shape, y_in.shape, y_out.shape

embedding_len = 128
MAX_LEN = max_len
vocab_size = len(count_words)

# Model for image feature extraction
img_model = Sequential()
img_model.add(Dense(embedding_len,input_shape=(2048,),activation='relu'))
img_model.add(RepeatVector(MAX_LEN))

img_model.summary()

# Model for generating captions from image features
captions_model = Sequential()
captions_model.add(Embedding(input_dim=vocab_size+1,output_dim=embedding_len,input_length=MAX_LEN))
captions_model.add(LSTM(256,return_sequences=True))
captions_model.add(TimeDistributed(Dense(embedding_len)))

captions_model.summary()

# Concatenating the outputs of image and caption models
concat_output = Concatenate()([img_model.output,captions_model.output])
# First LSTM Layer
output = LSTM(units=128,return_sequences=True)(concat_output)
# Second LSTM Layer
output = LSTM(units=512,return_sequences=False)(output)
# Output Layer 
output = Dense(units=vocab_size+1,activation='softmax')(output)
# Creating the final model
final_model = Model(inputs=[img_model.input,captions_model.input],outputs=output)
final_model.compile(loss='categorical_crossentropy',optimizer='RMSprop',metrics='accuracy')
final_model.summary()

mc = ModelCheckpoint('image_caption_generator.h5',monitor='accuracy',verbose=1,mode='max',save_best_only=True)

final_model.fit([X,y_in],
                y_out,
                batch_size=512,
                callbacks=mc,
                epochs=100)

# Creating an inverse dictionary with reverse key-value pairs
inverse_dict = {val: key for key,val in count_words.items()}

# Custom function for extracting an image and transforming it into an appropriate format
def getImage(idx):
    test_img_path = images[idx]
    test_img = cv2.imread(test_img_path)
    test_img = cv2.cvtColor(test_img,cv2.COLOR_BGR2RGB)
    test_img = cv2.resize(test_img,(224,224))
    test_img = np.reshape(test_img,(1,224,224,3))
    return test_img


#Evaluation Metrics  import cv2
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.sequence import pad_sequences
from evaluate import load
# Load the ROUGE metric
import evaluate
#from pycocoevalcap.spice.spice import Spice
#warnings.filterwarnings('ignore')
from nltk.translate.bleu_score import corpus_bleu
from queue import PriorityQueue

image_names = []
actual_captions = []
predicted_captions = []
bleu1_scores = []
bleu2_scores = []
bleu3_scores = []
bleu4_scores = []
rouge1_scores = []
rouge2_scores = []
rougeL_scores = []
rougeLsum_scores = []
beam_width = 3  # Define the beam width

for i in range(1):
    random_no = np.random.randint(0,1501,(1,1))[0,0]
    test_feature = model.predict(getImage(random_no)).reshape(1,2048)
    test_img_path = images[random_no]
    test_img = cv2.imread(test_img_path)
    test_img = cv2.cvtColor(test_img,cv2.COLOR_BGR2RGB)
    actual_captions_dict = {}

    # Load actual captions from captions.txt file
    with open('/Users/srisaijishnuedara/Documents/SEMESTER_6/ML_PROJECT/image_captioning/data/captions.txt', 'rt', encoding='utf-8') as file:
        for line in file:
            parts = line.strip().split(',')
            if len(parts) == 2:
                image_filename, caption = parts
                if image_filename in actual_captions_dict:
                    actual_captions_dict[image_filename].append(caption)
                else:
                    actual_captions_dict[image_filename] = [caption]

    # Get the matching image filename
    test_img_path=test_img_path.split("/")[-1]
    image_filename = match_image_name(test_img_path, actual_captions_dict)
    # test_feature = model.predict([getImage(random_no),actual_captions_dict[random_no]]).reshape(1,2048)
    if image_filename:
        actual_captions_list = actual_captions_dict[image_filename]
        actual_captions_str = ' '.join(actual_captions_list)
    else:
        print("No matching image filename found for:", test_img_path)

    # Initialize the beam with the start token
    beam = [{'sequence': ['startofseq'], 'score': 1.0}]

    # Generate captions using beam search
    for _ in range(50):  # Maximum caption length
        candidates = []
        for b in beam:
            # Encoding the current sequence
            encoded = [count_words[word] for word in b['sequence']]
            encoded = pad_sequences([encoded], maxlen=MAX_LEN, padding='post', truncating='post')

            # Predict the next word for the current sequence
            predictions = final_model.predict([test_feature, encoded])[0]

            # Get top-k predictions
            top_indices = np.argsort(predictions)[-beam_width:]

            # Generate candidate captions
            for idx in top_indices:
                candidate = {
                    'sequence': b['sequence'] + [inverse_dict[idx]],
                    'score': b['score'] * predictions[idx]
                }
                candidates.append(candidate)

        # Select top-k candidates based on score
        beam = sorted(candidates, key=lambda x: x['score'], reverse=True)[:beam_width]

        # Check for end token in the beam
        end_found = False
        for b in beam:
            if b['sequence'][-1] == 'endofseq':
                end_found = True
                break

        if end_found:
            break

    # Get the best caption from the beam
    pred_caption = beam[0]['sequence']
    pred_caption = pred_caption[1:-1]  # Remove start and end tokens
    pred_caption = ' '.join(pred_caption)
    print(f'Predicted Text:\n{pred_caption}')
    # Compute ROUGE scores
    rouge = evaluate.load('rouge')
    results = rouge.compute(predictions= [pred_caption], references=[actual_captions_str])
    print(f"The ROUGE score is: {results}")

    # Calculate BLEU scores
    Bleu1 = corpus_bleu([actual_captions_str], [pred_caption], weights=(1, 0, 0, 0))
   
    # Print BLEU scores
    print('Bleu_Score -1 = %f' % Bleu1)

   # Append data to lists
    image_names.append(image_filename)
    actual_captions.append(actual_captions_str)
    predicted_captions.append(pred_caption)
    bleu1_scores.append(Bleu1)
    rouge1_scores.append(results['rouge1'])
    rouge2_scores.append(results['rouge2'])
    rougeL_scores.append(results['rougeL'])
    rougeLsum_scores.append(results['rougeLsum'])

    # Create DataFrame
    data = {
        'Image Name': image_names,
        'Actual Caption': actual_captions,
        'Predicted Caption': predicted_captions,
        'BLEU1 Score': bleu1_scores,
        'ROUGE1 Score': rouge1_scores,
        'ROUGE2 Score': rouge2_scores,
        'ROUGEL Score': rougeL_scores,
        'ROUGELsum Score': rougeLsum_scores
    }

    df = pd.DataFrame(data)

    # Write DataFrame to CSV file
    df.to_csv('caption_scores_beam.csv', index=False)
    print(pred_caption)
    plt.figure(figsize=(5,5))
    plt.imshow(test_img)
    plt.xlabel(pred_caption)     #PREDICTING CAPTION FOR NEW IMAGE IN TELUGU  def translate_to_telugu(text):
    translator = Translator(to_lang="te")
    translation = translator.translate(text)
    return translation  import cv2
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load the pre-trained model and other necessary components
# ...

# Load the image you want to generate a caption for
new_image_path = "/Users/srisaijishnuedara/Documents/SEMESTER_6/ML_PROJECT/image_captioning/istockphoto-699899338-612x612.jpg"
new_image = cv2.imread(new_image_path)
new_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)

# Preprocess the new image
new_image_feature = model.predict(np.expand_dims(new_image, axis=0)).reshape(1, 2048)

# Initialize the beam with the start token
beam = [{'sequence': ['startofseq'], 'score': 1.0}]

# Generate a caption using beam search
for _ in range(50):  # Maximum caption length
    candidates = []
    for b in beam:
        # Encoding the current sequence
        encoded = [count_words[word] for word in b['sequence']]
        encoded = pad_sequences([encoded], maxlen=MAX_LEN, padding='post', truncating='post')

        # Predict the next word for the current sequence
        predictions = final_model.predict([new_image_feature, encoded])[0]

        # Get top-k predictions
        top_indices = np.argsort(predictions)[-beam_width:]

        # Generate candidate captions
        for idx in top_indices:
            candidate = {
                'sequence': b['sequence'] + [inverse_dict[idx]],
                'score': b['score'] * predictions[idx]
            }
            candidates.append(candidate)

    # Select top-k candidates based on score
    beam = sorted(candidates, key=lambda x: x['score'], reverse=True)[:beam_width]

    # Check for end token in the beam
    end_found = False
    for b in beam:
        if b['sequence'][-1] == 'endofseq':
            end_found = True
            break

    if end_found:
        break

# Get the best caption from the beam
pred_caption = beam[0]['sequence']
pred_caption = pred_caption[1:-1]  # Remove start and end tokens
pred_caption = ' '.join(pred_caption)
print(pred_caption)
pred_caption = translate_to_telugu(pred_caption)
print(pred_caption)
print("\n")
# Visualize the new image and the predicted caption
plt.figure(figsize=(5, 5))
plt.imshow(new_image)
plt.show()
